---
title: "Introduction to Regression Analysis"
subtitle: "Social Data Institute, Day 3"
author: "Michal Ov√°dek"
date: ""
format: revealjs
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)

```

## Today

- bivariate linear regression
- multiple regression
- interpreting coefficients
- statistical inference
- work with more data

## Students and the electoral register {.smaller}

Before 2015 in the UK, the head of the household could register all members of the household to vote. From 2015, all individuals had to register separately. There were particular concerns that this would lead to many students and young people `falling off' the electoral register. We collect data on voter registration in 573 UK constituencies to evaluate this concern.

- Unit of analysis: 573 parliamentary constituencies (all constituencies in England and Wales).

- Dependent variable (Y): *Change* in the number of registered voters in a constituency (from 2010 to 2015).

- Independent variable (X): Percentage of a constituency's population who are full time students.

## Data

:::: {.columns}

::: {.column width="50%"}

```{r, fig.height=6, fig.width=6}

library(scales)
library(data.table)

load("data/student_constituencies.Rdata")

constituencies$proportion.students <- (constituencies$proportion.young/2)*100

constituencies$proportion.students <- constituencies$proportion.students-min(constituencies$proportion.students)+1

constituencies$Electors_change <- constituencies$Electors_now - constituencies$Electors_then

constituencies$voters_change <- constituencies$Electors_change
constituencies$students <- constituencies$proportion.students

par(mfrow = c(1,1), mar = c(5,6,4,2)+0.1)
ylims <- round(range(constituencies$Electors_change)/1000)*1000
xlims <- range(c(0,constituencies$proportion.students,20))
plot(constituencies$proportion.students, constituencies$Electors_change, xlab = "Percentage of students", ylab = "", bty = "n", col = alpha("black", 0.4), pch = 19, cex = .8, cex.lab = 1.4, ylim = ylims, yaxt = "n", xlim = xlims)
axis(2, at = seq(ylims[1], ylims[2],by = 2000), las = 2)
mtext("Change in registered voters", side=2, line=5, cex = 1.4)

simple_ols_model <- lm(voters_change ~ students, 
                       data = constituencies)

```

:::

::: {.column width="50%"}
- What can we tell from looking at this plot?
- Is there a positive or a negative relationship between X and Y?
- Linear regression will help us to make more precise statements about relationships like this.
:::

::::

## What is a model?

- A model is a simplified abstraction of reality

- Typically, models are used to describe key features or dimensions of some more complicated process

- We do not want a model to capture *every* feature of reality, just those features that help us to describe the phenomena of interest

- "All models are wrong, but some are useful" -- George Box

- We will be using statistical models which will always be "wrong", but some will be useful

# The (Simple) Linear Regression Model

## Linear relationships

- The most straightforward way of describing the relationship between two variables is with a line

- A linear regression model is an approximation of the relationship between our independent variable X and our response variable Y

- In our case, a linear regression model will approximate the true relationship between: 

    - the proportion of students, and
    - the change in the number of registered voters

## Linear relationships

A line can be represented $Y = \alpha + \beta X$

:::: {.columns}

::: {.column width="50%"}

```{r, fig.width=7, fig.height=7}

### Alpha and beta
intercept <- .2
slope <- .7
cex.size <- 1.75

plot(0,0,col = "transparent", xlim = c(-2,2), ylim = c(-2,2), xlab = "X-axis", ylab = "Y-axis", main = bquote(alpha == .(intercept) ~ " and " ~ beta == .(slope)), cex.main = cex.size, cex.lab = cex.size, cex.axis = cex.size)
abline(h = 0, v= 0, lty = 3)
abline(intercept, slope, lwd = 3)
segments(x0 = 0, x1 = 0 , y0 = 0, y1 = intercept, col = "blue", lwd = 3)
text(x = 0.35, y = 0.1, labels = bquote(alpha == .(intercept)), col  ="blue", cex = 1.5)
segments(x0 = .5, x1 = 1.5 , y0 = intercept + .5*slope, y1 = intercept + .5*slope, col = "red", lwd = 3)
segments(x0 = 1.5, x1 = 1.5 , y0 = intercept + .5*slope, y1 = intercept + 1.5*slope, col = "red", lwd = 3)
text(x = 1.85, y = .85, labels = bquote(beta == .(slope)), col  ="red", cex = 1.5)

```
:::

::: {.column width="50%"}

- $\alpha$ is the intercept: the value of $Y$ where $X = 0$ 
- $\beta$ is the slope: the amount that $Y$ increases when $X$ increases by one unit 
- Here, a one-unit increase in $X$ is associated with a $0.7$-unit increase in $Y$
:::

::::

## Linear relationships (2)

:::: {.columns}

::: {.column width="50%"}

```{r, fig.width=7, fig.height=7}

### Alpha and beta
intercept <- .2
slope <- .7
cex.size <- 1.75

plot(0,0,col = "transparent", xlim = c(-2,2), ylim = c(-2,2), xlab = "X-axis", ylab = "Y-axis", main = bquote(alpha == .(intercept) ~ " and " ~ beta == .(slope)), cex.main = cex.size, cex.lab = cex.size, cex.axis = cex.size)
abline(h = 0, v= 0, lty = 3)
abline(intercept, slope, lwd = 3)
segments(x0 = 0, x1 = 0 , y0 = 0, y1 = intercept, col = "blue", lwd = 3)
text(x = 0.35, y = 0.1, labels = bquote(alpha == .(intercept)), col  ="blue", cex = 1.5)
segments(x0 = .5, x1 = 1.5 , y0 = intercept + .5*slope, y1 = intercept + .5*slope, col = "red", lwd = 3)
segments(x0 = 1.5, x1 = 1.5 , y0 = intercept + .5*slope, y1 = intercept + 1.5*slope, col = "red", lwd = 3)
text(x = 1.85, y = .85, labels = bquote(beta == .(slope)), col  ="red", cex = 1.5)

```

:::

::: {.column width="50%"}

```{r, fig.width=7, fig.height=7}

### Alpha and beta
intercept <- -.3
slope <- 1.2
cex.size <- 1.75

plot(0,0,col = "transparent", xlim = c(-2,2), ylim = c(-2,2), xlab = "X-axis", ylab = "Y-axis", main = bquote(alpha == .(intercept) ~ " and " ~ beta == .(slope)), cex.main = cex.size, cex.lab = cex.size, cex.axis = cex.size)
abline(h = 0, v= 0, lty = 3)
abline(intercept, slope, lwd = 3)
segments(x0 = 0, x1 = 0 , y0 = 0, y1 = intercept, col = "blue", lwd = 3)
text(x = -0.4, y = -0.15, labels = bquote(alpha == .(intercept)), col  ="blue", cex = 1.5)
segments(x0 = .5, x1 = 1.5 , y0 = intercept + .5*slope, y1 = intercept + .5*slope, col = "red", lwd = 3)
segments(x0 = 1.5, x1 = 1.5 , y0 = intercept + .5*slope, y1 = intercept + 1.5*slope, col = "red", lwd = 3)
text(x = 1.85, y = .85, labels = bquote(beta == .(slope)), col  ="red", cex = 1.5)

```

:::

::::



## The linear regression model {.smaller}

A simple way to summarize the relationship between two variables is to assume that they are linearly related.

We can express this with the simple linear regression model: 

$$ Y_i = \alpha +\beta X_i + \epsilon_i $$

- Observations $i = 1, \ldots , n$

- $Y$ is the dependent variable

- $X$ is the independent variable

- $\alpha$ ("alpha") is the intercept or constant

- $\beta$ ("beta") is the slope

- $\epsilon_i$ ("epsilon") is the error term or residual


## The linear regression model {.smaller}

$$ Y_i =  \alpha +\beta X_i + \epsilon_i $$

$\alpha$ and $\beta$ are the coefficients or parameters of the regression line.

- $\alpha$ gives the average value of Y when X is equal to 0
- $\beta$ gives the average change in Y that results from a 1-unit change in X 
- $\rightarrow$ describe the relationship that holds, on average, between X and Y

$\epsilon_i$ is the error term

- $\epsilon_i$ allows a unit to deviate from a perfect linear relationship
- $\rightarrow$ represents all factors aside from X that determine the value of Y

## The linear regression model (example) {.smaller}

In our voter registration example:

- $Y_i$ -- change in number of registered voters in constituency $i$
- $X_i$ -- percentage of students in constituency $i$
- $\epsilon_i$ -- all factors influencing registration other than student population

What does $\alpha$ represent? 

- the average change in registration for a constituency with 0% students

What does $\beta$ represent? 

- the average change in registration associated with a one unit change in the percentage of students


## What is a "one-unit" change? {.smaller}

If $\beta$ represents the change in Y that is associated with a a "one-unit" change in X, we need to know the units in which X is measured.

For example, a "one-unit" increase in...

- ...*age*, measured in years, is *one year*
- ...*height*, measured in inches, is *one inch*
- ...*GDP per capita*, measured in dollars, is *one dollar* 

Question: What is a one-unit increase in the "percentage of students"? 

Answer: A one **percentage point** increase in the percentage of students.

## "Percentage" versus "percentage point"

A frequent interpretation error is to confuse *percentage* changes with *percentage point* changes. What's the difference?

An increase in the percentage of students from 40% to 44% represents:

- An increase of 4 **percentage points**

- An increase of 10 **percent**

When including percentage variables in regression models, we will (almost) always speak about changes in **percentage points**.


## Unknown parameters {.smaller}

- $\alpha$ & $\beta$ represent the average relationship between $X$ and $Y$

    - They are population parameters -- values we assume exist in the world
    
- We would like to know the numerical values that $\alpha$ and $\beta$ take

- We don't know these values so we must estimate them

- We estimate the values of the parameters from the data

- We use a slightly different notation to indicate estimated parameters

    - $\alpha$ becomes $\hat{\alpha}$, which reads as "alpha hat"
    - $\beta$ becomes $\hat{\beta}$, which reads as "beta hat"


## Fitted values {.smaller}

We can also use the values of $\hat{\alpha}$ and $\hat{\beta}$ to calculate fitted or predicted values for any of our sample of X observations. 

The fitted values $\hat{Y}_i$ are:  
$$\hat{Y}_i = \hat{\alpha} + \hat{\beta} X_i, \ i=1, \dots, n$$

The fitted values tell us what the best guess is for Y for a specific value of X. 

The residuals $\hat{\epsilon}_i$ are 
$$\hat{\epsilon}_i = Y_i - \hat{Y}_i, \ i=1, \dots, n.$$

The residuals tell us how far our best guess for each observation is from the value of Y we observe in the sample.

## The linear regression line

```{r, fig.height=5.2, fig.width=4.5}

tmp <- constituencies[constituencies$proportion.students < 5,]

base_plot <- function(){

xlims <- range(c(tmp$proportion.students,0))
plot(tmp$proportion.students, tmp$Electors_change, xlab = "Percentage of students", ylab = "Change in registered voters", bty = "n", col = alpha("black",.2), pch = 19, cex = .8, xlim = xlims)

}

student_model <- lm(Electors_change ~ proportion.students, data = constituencies)

base_plot()

```

## Regression line

```{r, fig.height=5.2, fig.width=4.5}


base_plot()
abline(student_model, lwd = 3, col = "blue")


```

## Estimated alpha

```{r, fig.height=5.2, fig.width=4.5}


base_plot()
abline(student_model, lwd = 3, col = "blue")
mod_coefs <- coef(student_model)
abline(h = 0, lty = 3, lwd = 3)
abline(v = 0, lty = 3, lwd = 3)
segments(x0 = 0 , x1 = 0, y0 = 0, y1 = mod_coefs[1], col = "blue", lwd = 3)
text(x = 1, y = 2000, labels = expression(hat(alpha)), cex = 2, col = "blue")
arrows(x0 = .8, x1 = 0, y0 = 1600, y1 = mod_coefs[1], lwd = 2, length = .13)


```

## Estimated beta

```{r, fig.height=5.2, fig.width=4.5}


base_plot()
abline(student_model, lwd = 3, col = "blue")

x1 <- 2
x2 <- 3
abline(v = c(x1,x2), lty = 3, lwd = 2)
axis(1, at = c(2,3))

#segments(x0 = 0 , x1 = 0, y0 = 0, y1 = mod_coefs[1], col = "red", lwd = 3)
segments(x0 = x1 , x1 = x2, y0 = mod_coefs[1] + x2*mod_coefs[2], y1 = mod_coefs[1] + x2*mod_coefs[2], col = "blue", lwd = 3)
segments(x0 = x1 , x1 = x1, y0 = mod_coefs[1] + x1*mod_coefs[2], y1 = mod_coefs[1] + x2*mod_coefs[2], col = "blue", lwd = 3)

text(x = 1, y = -3000, labels = expression(hat(beta)), cex = 2, col = "blue")
arrows(x0 = 1.1, x1 = 2, y0 = -2800, y1 = mod_coefs[1] + x1*mod_coefs[2]  + mod_coefs[2]/2, lwd = 2, length = .13)



```

## Fitted vs observed Y

```{r, fig.height=5.2, fig.width=4.5}


base_plot()
abline(student_model, lwd = 3, col = "blue")

example_error <- tmp[which(tmp$Electors_change < -6000)[1],]

eps <- residuals(student_model)[which(constituencies$Codes == example_error$Codes)]
yhat <- predict(student_model, newdata = example_error)
y <- example_error$Electors_change
x <- example_error$proportion.students
abline(h = c(y, yhat), lty = 3, lwd = 2)
points(x = rep(x,2), y = c(y, yhat), pch = 19, cex =1.2, col = "blue")
text(x = x+.25, y = yhat + 1000, labels =bquote(hat(Y)[i]), col = "blue", cex = 2)
text(x = x+.25, y = y + 1000, labels =bquote(Y[i]), col = "blue", cex = 2)

```

## Error term

```{r, fig.height=5.2, fig.width=4.5}


base_plot()
abline(student_model, lwd = 3, col = "blue")

example_error <- tmp[which(tmp$Electors_change < -6000)[1],]

eps <- residuals(student_model)[which(constituencies$Codes == example_error$Codes)]
yhat <- predict(student_model, newdata = example_error)
y <- example_error$Electors_change
x <- example_error$proportion.students
abline(h = c(y, yhat), lty = 3, lwd = 2)
points(x = x, y = y, pch = 19, cex =1.2, col = "blue")
points(x = x, y = yhat, pch = 19, cex =1.2, col = "blue")
segments(x0 = x, y0 = y, y1 = yhat, col = "blue", lwd = 2)

text(x = x-.5, y = y - (y - yhat)/2, labels =bquote(hat(epsilon)[i]), col = "blue", cex = 2)
text(x = x+.25, y = yhat + 1000, labels =bquote(hat(Y)[i]), col = "blue", cex = 2)
text(x = x+.25, y = y + 1000, labels =bquote(Y[i]), col = "blue", cex = 2)

```


# Estimation and Interpretation

## Estimating $\alpha$ and $\beta$

The main goal of the simple regression model is to estimate a line that "fits" the data. Which of these lines best "fits" our data?

---

```{r, fig.width=5, fig.height=4, out.width = ".8\\linewidth", fig.align="center"}
set.seed(221186)

base_plot <- function(){
  par(mfrow = c(1,1), mar = c(5,6,4,2)+0.1)
ylims <- round(range(constituencies$Electors_change)/1000)*1000
xlims <- range(c(0,constituencies$proportion.students,20))
plot(constituencies$proportion.students, constituencies$Electors_change, xlab = "Percentage of students", ylab = "", bty = "n", col = alpha("black", 0.4), pch = 19, cex = .8, cex.lab = 1, ylim = ylims, yaxt = "n", xlim = xlims)
axis(2, at = seq(ylims[1], ylims[2],by = 2000), las = 2)
mtext("Change in registered voters", side=2, line=5, cex = 1)
}

tmp <- constituencies

coefs <- t(replicate(5,jitter(coef(student_model),3)))

base_plot()
for(i in 1:1) abline(coefs[i,], col = alpha("blue",.5), lwd = 2)

```

---

```{r, fig.width=5, fig.height=4, out.width = ".8\\linewidth", fig.align="center"}

base_plot()
for(i in 1:2) abline(coefs[i,], col = alpha("blue",.5), lwd = 2)

```

---

```{r, fig.width=5, fig.height=4, out.width = ".8\\linewidth", fig.align="center"}

base_plot()
for(i in 1:3) abline(coefs[i,], col = alpha("blue",.5), lwd = 2)

```

--- 

```{r, fig.width=5, fig.height=4, out.width = ".8\\linewidth", fig.align="center"}

base_plot()
for(i in 1:4) abline(coefs[i,], col = alpha("blue",.5), lwd = 2)

```

--- 

```{r, fig.width=5, fig.height=4, out.width = ".8\\linewidth", fig.align="center"}

base_plot()
for(i in 1:4) abline(coefs[i,], col = alpha("blue",.5), lwd = 2)
abline(student_model, col = alpha("blue",.5), lwd = 2)

```

## Ordinary Least Squares {.smaller}

- The most widely used approach to estimating the parameters of the linear regression model is the ordinary least squares (OLS) method.

- The OLS estimator chooses the regression coefficients so that the estimated line is as close as possible to the data 

- Formally, from all possible $\alpha$ and $\beta$ values, it chooses $\hat{\alpha}$ and $\hat{\beta}$ that minimize the sum of the squared residuals (SSR)

$$SSR = \sum_{i=1}^n \Big[ Y_i - \big(\hat{\alpha} + \hat{\beta} X_i\big)\Big]^2 =  \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$$

- OLS selects a line that makes the difference between the observed ($Y_i$) and fitted ($\hat{Y}_i$) values for each observation as small as possible


## Ordinary Least Squares (intuition)


```{r, fig.width=20, fig.height=8, out.width = "1.0\\linewidth", fig.align="center"}
x <- rnorm(20)
y <- rnorm(length(x), 0.1 + x*0.7)

ols_demo <- function(mod_coefs, no_line = F, return_sse = F){
par(cex.axis = 1.5, cex.lab = 1.5)
  y_hat <- mod_coefs[1] + x*mod_coefs[2]
  
  if(!return_sse){
  
  plot(x, y, pch = 19, col = alpha("black", 0.4), ylim = c(-3,3), xlim = c(-3,3), cex = 1.5)

  if(!no_line){
  abline(mod_coefs, lwd = 2)
  
for(i in 1:length(x)){
  
  segments(x0 = x[i], x1 = x[i], y0 = y[i], y1 = y_hat[i], lwd = 1.5)
  
}
  }
    
  }else{
   return(sum((y_hat - y)^2)) 
  }
  
    
}

par(mfrow = c(1,3), cex.axis = 1.2, cex.lab = 1.2, cex = 1.5)
ols_demo(c(.2,.1))
ols_demo(c(0.15,.4))
ols_demo(coef(lm(y ~ x)))

```

$\rightarrow$ OLS selects the line that *minimizes* the *sum of the squared distances* between each point and the line

## Ordinary Least Squares (formula)

When we have only two variables, we can apply two straightforward formulae to recover the OLS estimates: 

$$ 
\hat{\beta} = \frac{\sum_{i=1}^N (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^N(X_i - \bar{X})^2} = \frac{Cov(X,Y)}{Var(X)}
$$
$$ 
\hat{\alpha} = \bar{Y} - \hat{\beta}\bar{X} 
$$

where $\bar{X}$ and $\bar{Y}$ are the sample means of $X$ and $Y$.

## Estimate parameters

Now estimate the unknown parameters for our data in R.

- $Voters$ is the change in registered voters
- $Students$ is the % of students


## OLS estimates: interpretation

What is the interpretation of $\hat{\beta}$ = `r as.numeric(round(coef(simple_ols_model)[2],1))`?

- Generic: A one-unit increase in X is associated with a $\hat{\beta}$ change in Y, on average. 

- Specific: A one point increase in the percentage of students in a constituency is associated with a decrease of `r as.numeric(round(coef(simple_ols_model)[2]))` in the number of registered voters, on average. 

---

What is the interpretation of $\hat{\alpha}$ = `r as.numeric(round(coef(simple_ols_model)[1],1))`?

- Generic: $\hat{\alpha}$ is the average value of Y, when X is equal to 0 

- Specific: For a hypothetical constituency with 0 students, the model predicts that the number of registered  voters would increase by `r as.numeric(round(coef(simple_ols_model)[1]))` between 2010 and 2015.

- This interpretation of the intercept is not meaningful, as it extrapolates outside the range of the data.

## Make predictions

We can also calculate fitted values ($\hat{Y}_i = \hat{\alpha} + \hat{\beta} X_i$) for *any* arbitrary value of X which may be of interest.

```{r, echo=TRUE, eval=FALSE}
predict(model, newdata = data.frame(students = c(10,20)))
```

# Measures of fit

Measures of model fit help us to assess the degree to which our model approximates the real variation in our data.

$R^2$ measures the proportion of the variation in $Y_i$ that is explained by $X_i$. It varies between between 0 and 1 and can be used to describe how much of the variation in our dependent variable is "explained" by our independent variable.

- If X explains all the variation in Y, then $R^2 = 1$

- If X explains none of the variation in Y, then $R^2 = 0$

## R-squared

$R^2$ starts from the identity
$$
Y_i = \hat{Y}_i+\hat{\epsilon}_i
$$
where 

- $Y_i$ is the observed value of Y for observation $i$
- $\hat{Y}_i$ is the fitted value of Y for observation $i$
- $\hat{\epsilon}_i$ is the residual for observation $i$ ($\hat{\epsilon}_i \equiv Y_i - \hat{Y}_i$)


## R-squared {.smaller}

Imagine that we were to use a really dumb "model" to predict $Y$ for each value in our data:
$$\hat{Y}_{i\text{(dumb)}}= \bar{Y}$$

We could assess the accuracy of these "predictions" by calculating the distance between the predicted values and the observed values:
$$\text{TSS (Total Sum of Squares)} = \sum_{i=1}^n (Y_i - \hat{Y}_{i\text{(dumb)}})^2=\sum_{i=1}^n(Y_i-\bar{Y})^2 $$

The TSS is therefore the sum of the squared distances between each observation and the mean.

## R-squared {.smaller}

We can then compare the predictions from this dumb model, to the predictions (fitted values) from our regression model:
$$\hat{Y}_{i\text{(ols)}}= \hat{\alpha} + \hat{\beta}X_i$$

Again, let's calculate the accuracy by summing the distances between the predicted and observed values (i.e. the residuals):
$$\text{SSR (Sum of Squared Residuals)} = \sum_{i=1}^n (Y_i - \hat{Y}_{i\text{(ols)}})^2$$

If our regression model is doing a good job, we should make fewer or smaller prediction errors than when using the dumb model.

## R-squared {.smaller}

The $R^2$ is a statistic that summarises how much better the predictions from our regression model are relative to a baseline model where we just use the mean value of Y as a prediction for all observations (i.e. the dumb model)

Definition: 

The $R^2$ is defined as

$$
R^2 = \frac{TSS - SSR}{TSS}  = 1-\frac{SSR}{TSS}
$$

where

- TSS (Total sum of squares) equals	$\sum_{i=1}^n(Y_i-\bar{Y})^2$
- SSR (Sum squared residuals) equals $\sum_{i=1}^n(Y_i-\hat{Y}_i)^2$

Intuition: 

- $R^2$ varies between 0 and 1
- When the residuals (prediction errors) from our model are large (SSR is large), $R^2$ is closer to 0
- When the residuals (prediction errors) from our model are small (SSR is small), $R^2$ is closer to 1

## R-squared

```{r}

## R-square
set.seed(221186)
x <- rnorm(20)
y <- rnorm(length(x), .1 + x*.6)

base_plot <- function(...){
  plot(x,y, xlim = c(-3,3), ylim = c(-3,3), pch = 19, col = alpha("black",0.4),...)  
}


```

$R^2 = \frac{TSS - SSR}{TSS} = 1-\frac{SSR}{TSS}$

```{r, fig.width=12, fig.height=6, out.width = "1.0\\linewidth", fig.align="center"}

par(mfrow = c(1,2))

## High and low R-square
set.seed(221186)
x <- rnorm(30)
y <- rnorm(length(x), .1 + x*.8, sd = .5)

# Hi

base_plot(main = paste("High ",expression(R^2)))
abline(h = mean(y), lty = 1, lwd = 2)
y_hat <- mean(y)
tss <- sum((y - y_hat)^2)
segments(x0 = x, y0 = y_hat, y1 = y, lwd = 2)
abline(lm(y ~ x), col = "red", lwd = 2)
y_hat <- predict(lm(y ~ x))
segments(x0 = x, y0 = y_hat, y1 = y, col = "red", lwd = 2)
ess <- sum((mean(y) - y_hat)^2)


# Lo

set.seed(221186)
x <- rnorm(30)
y <- rnorm(length(x), .1 + x*.2, sd = 1)


base_plot(main = paste("Low ",expression(R^2)))
abline(h = mean(y), lty = 1, lwd = 2)
y_hat <- mean(y)
tss <- sum((y - y_hat)^2)
segments(x0 = x, y0 = y_hat, y1 = y, lwd = 2)
abline(lm(y ~ x), col = "red", lwd = 2)
y_hat <- predict(lm(y ~ x))
segments(x0 = x, y0 = y_hat, y1 = (y), col = "red", lwd = 2)
ess <- sum((mean(y) - y_hat)^2)

```

## How useful is $R^2$?

What does $R^2$ tell us?

- Large values $\rightarrow$ independent variable is good at predicting Y

- Small values $\rightarrow$ independent variable is poor at predicting Y 

What does $R^2$ not tell us?

- Large $R^2$ does not imply a causal relationship

- Low $R^2$ does not necessarily imply a useless regression


